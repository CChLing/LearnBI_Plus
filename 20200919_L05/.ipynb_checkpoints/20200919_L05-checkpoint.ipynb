{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导师制名企实训班商业智能方向 004期 Lesson 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking 1: 在CTR点击率预估中，使用GBDT+LR的原理是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在点击率预估中，GBDT+LR的方式是利用GBDT来进行特征抽取和特征生成，将GBDT输出的种类进行OneHot编码作为特征输入到LR中，LR是用来学习特征组合和进行点击率预估的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking 2: Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wide&Deep模型有单层Wide部分和多层Deep部分组合而成，单层线性的Wide部分能够让模型学习训练数据中的高频共现的特征，这种能力类似记忆能力，多层非线性的Deep部分可以学习训练数据中特征组合的潜在关联，从而对于在训练数据中未出现过的内容拥有一定推断能力，这种能力使得模型具有泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking 3: 在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 并行结构，如DeepFM，通过将FM和DNN平行计算，最后结合FM和DNN输出的结果进行CTR预估。\n",
    "2. 串行结构，如NFM，将FM输出的结果作为DNN的输入；如NeuMF，将DNN的输出作为FM的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking 4: GBDT和随机森林都是基于树的算法，它们有什么区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\">\n",
    "<tr>\n",
    "    <td><b>GBDT</b></td><td><b>随机森林</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Boosting思想</td><td>Bagging思想</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>由回归树构成</td><td>可由分类树构成，也可以由回归树构成</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>只能串行生成树</td><td>可并行生成树</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>结果由多棵树结果累加</td><td>结果由多棵树结果投票等</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>对异常值敏感</td><td>对异常值不敏感</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>是弱分类器的集成</td><td>对训练集一视同仁</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>通过减少模型偏差提高性能</td><td>通过减少模型方差提高性能</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking 5: item流行度在推荐系统中有怎样的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 冷启动问题：可以一定程度上解决冷启动问题，在新用户加入系统的时候，可以给用户推荐热门商品。\n",
    "2. 个性化推荐：在个性化推荐过程中，为了更好地挖掘用户的兴趣，可以降低流行度较高的item对于用户个性化预测的影响。\n",
    "3. 考虑不同应用场景：如果要打造爆款（如唯品会），就需要进行热门推荐，如果强调个性化甚至需要逆大众化（如婚恋网站）需要挖掘流行度较低的item。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action 1: 使用Wide&Deep模型对movielens进行评分预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#引包\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# deepctr\n",
    "from deepctr.models import WDL\n",
    "from deepctr.feature_column import SparseFeat,get_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "data = pd.read_csv(\"data/movielens_sample.txt\")\n",
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = ['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对特征标签进行编码\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个特征中的 不同特征值的个数\n",
    "fixlen_feature_columns = [SparseFeat(feature, data[feature].nunique()) for feature in sparse_features]\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集切分成训练集和测试集\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "train_model_input = {name:train[name].values for name in feature_names}\n",
    "test_model_input = {name:test[name].values for name in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\learnBI\\lib\\site-packages\\deepctr\\layers\\utils.py:171: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 128 samples, validate on 32 samples\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 6s 48ms/step - loss: 13.4374 - mean_squared_error: 13.4374 - val_loss: 15.3617 - val_mean_squared_error: 15.3617\n"
     ]
    }
   ],
   "source": [
    "# 使用WDL进行训练\n",
    "model = WDL(linear_feature_columns, dnn_feature_columns, task='regression')\n",
    "model.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "history = model.fit(train_model_input, train[target].values, batch_size=256, epochs=1, verbose=True, validation_split=0.2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用WDL进行预测\n",
    "pred_ans = model.predict(test_model_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE 3.9082860693659565\n",
      "test MSE 15.2747\n"
     ]
    }
   ],
   "source": [
    "# 输出RMSE或MSE\n",
    "mse = round(mean_squared_error(test[target].values, pred_ans), 4)\n",
    "rmse = mse ** 0.5\n",
    "print(\"test RMSE\", rmse)\n",
    "print(\"test MSE\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### movielens-1m数据集\n",
    "These files contain 1,000,209 anonymous ratings of approximately 3,900 movies \n",
    "made by 6,040 MovieLens users who joined MovieLens in 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MovieLens数据集预处理\n",
    "def movie_lens_preprocess(ratings_file, users_file, movies_file, \n",
    "                          rating_col=\"UserID::MovieID::Rating::Timestamp\",\n",
    "                          user_col=\"UserID::Gender::Age::Occupation::Zip-code\", \n",
    "                          movie_col=\"MovieID::Title::Genres\"):\n",
    "    ratings = pd.read_csv(ratings_file, header=None, sep=\"::\", engine='python')\n",
    "    ratings.columns=rating_col.split(\"::\")\n",
    "    movies = pd.read_csv(movies_file, header=None, sep=\"::\", engine='python')\n",
    "    movies.columns=movie_col.split(\"::\")\n",
    "    users = pd.read_csv(users_file, header=None, sep=\"::\", engine='python')\n",
    "    users.columns=user_col.split(\"::\")\n",
    "    data = pd.merge(ratings, movies,how=\"left\", on=\"MovieID\")\n",
    "    data = pd.merge(data, users, how=\"left\", on=\"UserID\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wide&Deep模型训练和预测\n",
    "def WDL_train_predict(data, sparse_features, target):\n",
    "    # 对特征标签进行编码\n",
    "    for feature in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feature] = lbe.fit_transform(data[feature])\n",
    "    # 计算每个特征中的 不同特征值的个数\n",
    "    fixlen_feature_columns = [SparseFeat(feature, data[feature].nunique()) for feature in sparse_features]\n",
    "    print(fixlen_feature_columns)\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "    # 将数据集切分成训练集和测试集\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "    train_model_input = {name:train[name].values for name in feature_names}\n",
    "    test_model_input = {name:test[name].values for name in feature_names}\n",
    "    # 使用Wide&Deep进行训练\n",
    "    model = WDL(linear_feature_columns, dnn_feature_columns, task='regression')\n",
    "    model.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "    history = model.fit(train_model_input, train[target].values, batch_size=256, epochs=1, verbose=True, validation_split=0.2, )\n",
    "    # 使用Wide&Deep进行预测\n",
    "    pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "    # 输出RMSE或MSE\n",
    "    mse = round(mean_squared_error(test[target].values, pred_ans), 4)\n",
    "    rmse = mse ** 0.5\n",
    "    print(\"\\n\\n\",\"*\"*150)\n",
    "    print(\"test RMSE\", rmse)\n",
    "    print(\"test MSE\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseFeat(name='MovieID', vocabulary_size=3706, embedding_dim=4, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.RandomNormal object at 0x00000211D05E89B0>, embedding_name='MovieID', group_name='default_group', trainable=True), SparseFeat(name='UserID', vocabulary_size=6040, embedding_dim=4, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.RandomNormal object at 0x00000211D2F80EB8>, embedding_name='UserID', group_name='default_group', trainable=True), SparseFeat(name='Gender', vocabulary_size=2, embedding_dim=4, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.RandomNormal object at 0x00000211D2F80518>, embedding_name='Gender', group_name='default_group', trainable=True), SparseFeat(name='Age', vocabulary_size=7, embedding_dim=4, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.RandomNormal object at 0x00000211D2F80BE0>, embedding_name='Age', group_name='default_group', trainable=True), SparseFeat(name='Occupation', vocabulary_size=21, embedding_dim=4, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.RandomNormal object at 0x00000211D2F80D30>, embedding_name='Occupation', group_name='default_group', trainable=True), SparseFeat(name='Zip-code', vocabulary_size=3439, embedding_dim=4, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.RandomNormal object at 0x00000211D2F80128>, embedding_name='Zip-code', group_name='default_group', trainable=True)]\n",
      "Train on 640133 samples, validate on 160034 samples\n",
      "Epoch 1/1\n",
      "640133/640133 [==============================] - 16s 25us/step - loss: 1.0119 - mean_squared_error: 1.0112 - val_loss: 0.8286 - val_mean_squared_error: 0.8274\n",
      "\n",
      "\n",
      " ******************************************************************************************************************************************************\n",
      "test RMSE 0.9086803618434812\n",
      "test MSE 0.8257\n"
     ]
    }
   ],
   "source": [
    "data = movie_lens_preprocess(\"data/ml-1m/ratings.dat\", \n",
    "                             \"data/ml-1m/users.dat\", \n",
    "                             \"data/ml-1m/movies.dat\")\n",
    "WDL_train_predict(data, [\"MovieID\", \"UserID\", \"Gender\", \"Age\", \"Occupation\", \"Zip-code\"], ['Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnBI",
   "language": "python",
   "name": "learnbi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
